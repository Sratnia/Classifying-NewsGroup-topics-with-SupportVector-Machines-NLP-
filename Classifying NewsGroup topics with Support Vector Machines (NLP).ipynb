{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514bc011",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afff991",
   "metadata": {},
   "source": [
    "# Libraries required for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f2539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Ashutosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ashutosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ashutosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ashutosh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "plt.style.use(\"ggplot\")\n",
    "nltk.download('names')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e280b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe14a74",
   "metadata": {},
   "source": [
    "## Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the keys from given dataset\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Description\n",
    "print(data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a67a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type of target categories \n",
    "data[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual text data\n",
    "data[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5efb87",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da47a6f",
   "metadata": {},
   "source": [
    "# So, the basic workflow of our project will go like\n",
    "\n",
    "# Data reading-->Data cleaning(In this case, text cleaning using NLP techniques)-->Data Transformation(To model understandable type)--> Model Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f6bbf",
   "metadata": {},
   "source": [
    "## Dividing our dataset into training and testing...\n",
    "\n",
    "## We will be working with only 3 categories to predict..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf309b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our target variables\n",
    "target=['sci.electronics','rec.sport.hockey','talk.politics.guns']\n",
    "#Training data\n",
    "df_train= fetch_20newsgroups(subset='train', categories=target, random_state=101)\n",
    "#Testing data\n",
    "df_test= fetch_20newsgroups(subset='test', categories=target, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb03c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72694fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142c8be",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ef770",
   "metadata": {},
   "source": [
    "## So, now we will be cleaning our datasets by removing unwanted characters like punctuations, stopwords, and lemmatizing different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adafb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be creating a function to clean our textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f489787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK- natural language toolkit\n",
    "stop_words= stopwords.words('english')# stopwords- a, the, an ,for, is...etc\n",
    "#all_names\n",
    "all_names=set(names.words())\n",
    "# preprocessing\n",
    "#- lower case\n",
    "#- root form run, running- run (lemmatization)\n",
    "\n",
    "lemma= WordNetLemmatizer()\n",
    "def is_letter_only(word):\n",
    "    return word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definign a fucntion to clean our text data for both training and testing dataset...by lemmatizing and removing stopwords...\n",
    "def clean_text(doc):\n",
    "    doc_clean=[]\n",
    "    for i in doc:\n",
    "        i=i.lower()\n",
    "        i_clean=' '.join(lemma.lemmatize(word) for word in i.split() if is_letter_only(word) and word not in all_names and word not in stop_words)\n",
    "        doc_clean.append(i_clean)\n",
    "    return doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a20c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be cleaning our data\n",
    "#clean train data\n",
    "df_train_clean= clean_text(df_train.data)\n",
    "df_train_label= df_train.target\n",
    "\n",
    "#clean test data\n",
    "df_test_clean= clean_text(df_test.data)\n",
    "df_test_label= df_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac952a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting our target values\n",
    "print(\"Training label\",Counter(df_train_label))\n",
    "print(\"Testing label\",Counter(df_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af408592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now our text data is clean and ready to be converted to numeric form by using tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer(stop_words='english') #basically stopping all the english basic words\n",
    "df_train_conv= tfidf.fit_transform(df_train_clean)\n",
    "df_test_conv= tfidf.transform(df_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb2f13",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04f15c",
   "metadata": {},
   "source": [
    "## Now our data is clean and ready to perform predictive analysis using SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f417c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Our SVC model\n",
    "model= SVC() #default model\n",
    "model.fit(df_train_conv, df_train_label) #model fitting with train data\n",
    "acc= model.score(df_test_conv,df_test_label)\n",
    "print(\"The accuracy of binary classification is : {}%\".format(round(acc*100,2))) #Model accuracy score\n",
    "\n",
    "#Model predictions\n",
    "y_pred=model.predict(df_test_conv)\n",
    "\n",
    "#Model Evaluation\n",
    "print(classification_report(df_test_label, y_pred))\n",
    "print(confusion_matrix(df_test_label, y_pred))\n",
    "plot_confusion_matrix(model,df_test_conv,df_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54fd04",
   "metadata": {},
   "source": [
    "## So, Our model performed with an overall accuracy of 97.3%..Similarly, We can peform with multi-class as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f08fd",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
